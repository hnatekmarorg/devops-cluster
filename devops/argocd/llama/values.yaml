# Llama models deployed by this chart:
# - exl (ctx: 2048, engine: tabby, replicas: 1, gpu: 6, auth: disabled)
# - cogito (ctx: 2048, temp: 0.6, top_p: 0.9, min_p: 0.01, engine: tabby, gpu: 6)
# - gpt-oss (ctx: 131072, temp: 1.0, top_p: 1, top_k: 0, engine: tabby, gpu: 4)
# - gpt-oss-20b (engine: vllm, gpu: 1)
# - glm-4-6 (ctx: 106000, top_p: 0.95, top_k: 40, engine: tabby, gpu: 6)
# - behemoth (ctx: 80000, engine: tabby, gpu: 6)
# - behemoth-redux (ctx: 64000, engine: tabby, gpu: 6)
# - glm-air (engine: vllm, gpu: 4)
# - qwen3-235b-thinking (ctx: 226000, engine: tabby, gpu: 6)
# - qwen3-235b-instruct (ctx: 226000, engine: tabby, gpu: 6)
# - qwen3-30b-instruct (engine: vllm, gpu: 2)
# - qwen3-30b-thinking (engine: vllm, gpu: 2)
# - qwen-coder (engine: vllm, gpu: 4)
# - qwen-next-instruct (engine: vllm, gpu: 6)
# - qwen-embedding (engine: vllm, gpu: 1)
# - qwen-reranker (engine: vllm, gpu: 1)

# Llama models configuration
# Each model defines GPU resources, arguments, and other settings
models:
  - name: glm-4-6V-flash
    image: gitea.hnatekmar.xyz/public/vllm:glm-4-6-flash
    engine: vllm
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp16/GLM-4.6V-Flash/ --tensor-parallel-size 4 --tool-call-parser glm45 --reasoning-parser glm45 --enable-auto-tool-choice  --allowed-local-media-path / --mm-encoder-tp-mode data --served-model-name glm-4-6V-flash

  - name: deepseek-ocr
    image: vllm/vllm-openai:v0.11.1
    engine: vllm
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp16/DeepSeek-OCR --logits_processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor --no-enable-prefix-caching --mm-processor-cache-gb 0 --served-model-name deepseek-ocr
  - name: hy-mt
    image: vllm/vllm-openai:v0.12.0
    engine: vllm
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp16/HY-MT1.5-1.8B/ --served-model-name hy-mt

  # ExL model configuration
  - name: exl
    cooldownPeriod: 3600
    engine: tabby
    replicas: 1
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - main.py
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model-dir
      - /models/exl2
      - --inline-model-loading
      - "true"
      - --disable-auth
      - "true"
  - name: precog
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - -m 
      - /models/gguf/precog/TheDrummer_Precog-123B-v1-Q5_K_M-00001-of-00003.gguf 
      - --ctx-size
      - "131072"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - distribute
      - --alias
      - "precog"
  - name: qwen3-32b
    cooldownPeriod: 3600
    engine: vllm
    image: vllm/vllm-openai:v0.12.0
    env:
      - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
        value: "1"
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-32B-FP8/
      - -tp
      - "4"
      - --gpu-memory-util
      - "0.86"
      - --max-model-len
      - "131072"
      - --enable-auto-tool-choice
      - --tool-call-parser
      - hermes
      - --reasoning-parser
      - qwen3
      - --served-model
      - qwen3-32b
      - --hf-overrides
      - |
        '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}'
  - name: precog-24b
    cooldownPeriod: 3600
    engine: vllm
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
    args:
      - /models/fp8/Precog-24B-v1_Compressed-Tensors --max-model-len 110000 -tp 2 --served-model-name precog-24b
  # GPT-OSS model configuration
  - name: gpt-oss
    engine: vllm
    cooldownPeriod: 3600
    image: vllm/vllm-openai:v0.11.1
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/gpt-oss-120b/ -tp 4  --enable-expert-parallel --tool-call-parser openai --served-model-name gpt-oss --gpu-memory-util 0.86
      
  - name: gpt-oss-derestricted
    engine: vllm
    cooldownPeriod: 3600
    image: vllm/vllm-openai:v0.12.0
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/gpt-oss-120b-Derestricted-MXFP4/ -tp 4  --enable-expert-parallel --tool-call-parser openai --served-model-name gpt-oss-derestricted  --gpu-memory-util 0.86

  # GPT-OSS 20B model configuration
  - name: gpt-oss-20b
    engine: vllm
    image: vllm/vllm-openai:v0.11.1
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp8/gpt-oss-20b
      - --tool-call-parser 
      - openai
      - --enable-auto-tool-choice
      - --served-model-name
      - gpt-oss-20b
  - name: deepseek-r1
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "8096"
      - --temp 
      - "0.6"
      - --top-p
      - "0.95"
      - --jinja
      - "--fit"
      - "on"
      - -m
      - "/models/gguf/deepseek/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf"
      - -ctk 
      - "q8_0"
      - -fa 
      - "on"
      - --alias
      - deepseek-r1
  # GLM-4.6 model configuration
  - name: glm-4-6
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "64000"
      - --temp 
      - "1.0"
      - --top-p
      - "0.95"
      - --top-k
      - "40"
      - --jinja
      - "--fit"
      - "on"
      - -m
      - "/models/gguf/glm-4.6/GLM-4.6-UD-Q4_K_XL-00001-of-00005.gguf"
      - --alias
      - glm-4-6
  - name: glm-4-7
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "64000"
      - --temp 
      - "1.0"
      - --top-p
      - "0.95"
      - --jinja
      - "--fit"
      - "on"
      - -m
      - "/models/gguf/glm-4.7/GLM-4.7-UD-Q4_K_XL-00001-of-00005.gguf"
      - --alias
      - glm-4-7
  - name: glm-steam
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - -m 
      - /models/gguf/glm-steam/TheDrummer_GLM-Steam-106B-A12B-v1-Q6_K-00001-of-00003.gguf 
      - --fit
      - "on"
      - -ngl 
      - "99"
      - --alias
      - glm-steam
  # Behemoth model configuration
  - name: behemoth
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "80000"
      - --no-mmap
      - "-m"
      - "/models/gguf/behemoth-r1/TheDrummer_Behemoth-R1-123B-v2-Q5_K_M-00001-of-00003.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - numactl
      - --alias
      - "behemoth"

  - name: minimax-m2
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "128000"
      - --temp
      - "1.0"
      - --top-p
      - "0.95"
      - --top-k
      - "40"
      - --no-mmap
      - "-m"
      - "/models/gguf/minimax-m2/cerebras_MiniMax-M2-REAP-162B-A10B-Q4_K_S-00001-of-00003.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --jinja
      - --numa
      - numactl
      - --alias
      - "minimax-m2"
  # Behemoth Redux model configuration
  - name: qwen3-235b-thinking
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "212992"
      - --jinja
      - --no-mmap
      - -m
      - /models/gguf/qwen3-235B/Qwen_Qwen3-235B-A22B-Thinking-2507-IQ3_XS-00001-of-00003.gguf
      - --top-p
      - "0.95"
      - --top-k
      - "20"
      - --temp 
      - "0.6"
      - --min-p 
      - "0.0"
      - --flash-attn 
      - "on"
      - --presence-penalty 
      - "0"
      - --numa
      - distribute
      - --alias
      - "qwen3-235b-thinking"
  - name: monstral
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "64000"
      - --no-mmap
      - "-m"
      - "/models/gguf/monstral/Monstral-123B-v2-Q5_K_M-00001-of-00003.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - distribute
      - --alias
      - "monstral"
  # GLM-Air model configuration
  - name: glm-air
    engine: vllm
    image: vllm/vllm-openai:v0.12.0
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/awq/GLM-4.5-Air-AWQ-4bit/
      - --tensor-parallel-size
      - "2"
      - --pipeline-parallel-size
      - "2"
      - --enable-expert-parallel
      - --enable-auto-tool-choice 
      - --max-num-seqs
      - "64"
      - --max-model-len
      - "120000"
      - --tool-call-parser
      - glm45
      - --reasoning-parser
      - glm45
      - --served-model-name
      - glm-air
  - name: glm-air-q6
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "0"
      - --no-mmap
      - "-np"
      - "2"
      - "-m"
      - "/models/gguf/glm-air/GLM-4.5-Air-Q6_K-00001-of-00002.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - numactl
      - --alias
      - "glm-air-q6"
  # Qwen3-30B Thinking model configuration
  - name: qwen3-30b-vl-thinking
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-VL-30B-A3B-Thinking-FP8/
      - -pp
      - "2"
      - -tp
      - "2"
      - --mm-encoder-tp-mode
      - data
      - --enable-expert-parallel
      - --reasoning-parser
      - deepseek_r1
      - --tool-call-parser
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-vl-thinking
  # Qwen3-30B Instruct model configuration
  - name: qwen3-30b-vl-instruct
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-VL-30B-A3B-Instruct-FP8/
      - -pp
      - "2"
      - -tp
      - "2"
      - --mm-encoder-tp-mode
      - data
      - --enable-expert-parallel
      - --tool-call-parser
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-vl-instruct
  - name: qwen3-30b-instruct
    engine: vllm
    image: vllm/vllm-openai:v0.11.1
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
    args:
      - /models/fp8/Qwen3-30B-A3B-Instruct-2507-FP8/
      - -tp
      - "2"
      - --max-model-len
      - "80000"
      - --gpu-memory-util 
      - "0.85"
      - --enable-expert-parallel
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-instruct
  # Qwen3-30B Thinking model configuration
  - name: qwen3-30b-thinking
    engine: vllm
    image: vllm/vllm-openai:v0.11.1
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-30B-A3B-Thinking-2507-FP8/
      - -tp
      - "4"
      - --gpu-memory-util
      - "0.85"
      - --enable-expert-parallel
      - --reasoning-parser 
      - deepseek_r1
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-thinking
  # Qwen Coder model configuration
  - name: qwen-coder
    engine: vllm
    cooldownPeriod: 14400
    env:
      - name: VLLM_ATTENTION_BACKEND
        value: FLASHINFER
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
    image: vllm/vllm-openai:v0.13.0
    args:
      - /models/fp8/Qwen3-Coder-30B-A3B-Instruct-FP8/  --tensor-parallel-size 2 --max-num-seqs 4 --max-model-len 16384 --gpu-memory-util 0.85 --kv-cache-dtype fp8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 4096 --served-model-name qwen-coder --disable-log-requests
  # Qwen Next Instruct model configuration
  - name: qwen-next-instruct
    engine: vllm
    cooldownPeriod: 14400
    image: vllm/vllm-openai:v0.12.0
    env:
      - name: CUDA_DEVICE_ORDER
        value: "PCI_BUS_ID"             # Fixes GPU indexing
      - name: VLLM_USE_CUSTOM_ALL_REDUCE
        value: "0"                      # Stability for mixed cards
      - name: NCCL_P2P_DISABLE
        value: "1"                      # Stability for mixed cards
      - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
        value: "300"                    # Increases timeout tolerance
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - /models/fp8/Qwen3-Next-80B-A3B-Instruct-FP8/  --pipeline-parallel-size 3 --tensor-parallel-size 2 --distributed-executor-backend mp --trust-remote-code  --enable-auto-tool-choice --tool-call-parser hermes
      - --served-model-name
      - qwen-next-instruct
        
  - name: qwen-next-thinking
    engine: vllm
    cooldownPeriod: 14400
    image: vllm/vllm-openai:v0.12.0
    env:
      - name: CUDA_DEVICE_ORDER
        value: "PCI_BUS_ID"             # Fixes GPU indexing
      - name: VLLM_USE_CUSTOM_ALL_REDUCE
        value: "0"                      # Stability for mixed cards
      - name: NCCL_P2P_DISABLE
        value: "1"                      # Stability for mixed cards
      - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
        value: "300"                    # Increases timeout tolerance
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - /models/fp8/Qwen3-Next-80B-A3B-Thinking-FP8/  --pipeline-parallel-size 3 --tensor-parallel-size 2    --distributed-executor-backend mp     --trust-remote-code   --served-model-name qwen-next-thinking --enable-auto-tool-choice     --tool-call-parser hermes --reasoning-parser deepseek_r1
  - name: qwen-next-thinking-tools
    engine: vllm
    cooldownPeriod: 14400
    image: vllm/vllm-openai:v0.12.0
    env:
      - name: CUDA_DEVICE_ORDER
        value: "PCI_BUS_ID"             # Fixes GPU indexing
      - name: VLLM_USE_CUSTOM_ALL_REDUCE
        value: "0"                      # Stability for mixed cards
      - name: NCCL_P2P_DISABLE
        value: "1"                      # Stability for mixed cards
      - name: VLLM_ENGINE_ITERATION_TIMEOUT_S
        value: "300"                    # Increases timeout tolerance
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - /models/fp8/Qwen3-Next-80B-A3B-Thinking-FP8/  --pipeline-parallel-size 3 --tensor-parallel-size 2 --distributed-executor-backend mp     --trust-remote-code   --served-model-name qwen-next-thinking-tools --enable-auto-tool-choice     --tool-call-parser hermes
  - name: qwen3-32b-thinking
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-VL-32B-Thinking-FP8/
      - --mm-processor-cache-gb 
      - "0"
      - --limit-mm-per-prompt.video
      - "0"
      - --max-model-len
      - "150000"
      - --tensor-parallel-size
      - "4"
      - --reasoning-parser 
      - deepseek_r1
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-32b-thinking
  - name: qwen3-32b-instruct
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-VL-32B-Instruct-FP8/
      - --mm-processor-cache-gb 
      - "0"
      - --limit-mm-per-prompt.video
      - "0"
      - --tensor-parallel-size
      - "4"
      - --max-model-len
      - "150000"
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-32b-instruct
  - name: kimi-linear
    engine: vllm
    image: vllm/vllm-openai:v0.11.1
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/awq/Kimi-Linear-48B-A3B-Instruct-AWQ-8bit/ --served-model-name kimi-linear --tensor-parallel-size 4   --trust-remote-code --enable-expert-parallel --max-model-len 903000
  - name: nemotron-3-nano
    engine: vllm
    image: vllm/vllm-openai:v0.12.0
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp16/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/ --mamba-ssm-cache-dtype float32 -tp 4 --trust-remote-code --trust-remote-code --enable-auto-tool-choice   --tool-call-parser qwen3_coder   --reasoning-parser-plugin /models/fp16/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/nano_v3_reasoning_parser.py   --reasoning-parser nano_v3  --served-model-name nemotron-3-nano
  - name: qwen3-8b-instruct
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp8/Qwen3-VL-8B-Instruct-FP8/
      - --limit-mm-per-prompt.video
      - "0"
      - --mm-encoder-tp-mode 
      - data
      - --max-model-len
      - "50000"
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-8b-instruct
  - name: qwen3-8b-thinking
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp8/Qwen3-VL-8B-Thinking-FP8/
      - --limit-mm-per-prompt.video
      - "0"
      - --gpu-memory-utilization 
      - "0.96"
      - --mm-encoder-tp-mode 
      - data
      - --max-model-len
      - "50000"
      - --mm-processor-cache-type
      - shm
      - --reasoning-parser 
      - deepseek_r1
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-8b-thinking
  - name: qwen3-4b-instruct
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp8/Qwen3-VL-4B-Instruct-FP8/
      - --mm-processor-cache-gb 
      - "0"
      - --limit-mm-per-prompt.video
      - "0"
      - --gpu-memory-utilization 
      - "0.96"
      - --mm-encoder-tp-mode 
      - data
      - --max-model-len
      - "100000"
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-4b-instruct
  - name: l3-electra
    engine: vllm
    image: vllm/vllm-openai:v0.12.0
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/L3.3-Electra-R1-70b-FP8-Dynamic/ -tp 4 --max-num-seqs 2 --max-model-len 48000
      - --served-model-name
      - l3-electra
  - name: devstral-small-2
    engine: vllm
    image: vllm/vllm-openai:v0.13.0
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp16/Devstral-Small-2-24B-Instruct-2512/ --max-model-len 262144 --tensor-parallel-size 4 --tool-call-parser mistral --enable-auto-tool-choice
      - --served-model-name
      - devstral-small-2
