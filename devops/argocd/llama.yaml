apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llama-cpp
  namespace: argocd  # or your Argo CD namespace
spec:
  project: default
  ignoreDifferences:
    - kind: "*"
      group: "*"
      namespace: inference
      jsonPointers:
        - /spec/replicas
  sources:
    - name: chart
      repoURL: https://github.com/hnatekmarorg/devops-cluster.git
      path: charts/llama-cpp
      targetRevision: main
      helm:
        valuesObject:
          models:
            - name: exl
              cooldownPeriod: 3600
              engine: tabby
              replicas: 1
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - main.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --cuda-malloc-backend
                - "true"
                - --model-dir
                - /models/exl2
                - --inline-model-loading
                - "true"
                - --disable-auth
                - "true"
              cooldownPeriod: 600
            - name: llama-3-nemotron
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 3
                limits:
                  nvidia.com/gpu: 3
              args:
                - -c
                - "32768" 
                - -sm 
                - row
                - --temp
                - "0.6"
                - --top-p
                - "0.95"
                - --top-k
                - "20"
                - --no-mmap
                - --jinja
                - --model
                - "/models/gguf/nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-Q6_K_L.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "llama-3-nemotron"
            - name: qwen-32b
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 3
                limits:
                  nvidia.com/gpu: 3
              args:
                - -c
                - "40960" 
                - -n
                - "32768"
                - -sm 
                - row
                - --rope-scaling 
                - yarn 
                - --rope-scale
                - "4"
                - --yarn-orig-ctx
                - "32768"
                - --temp
                - "0.6"
                - --top-p
                - "0.95"
                - --top-k
                - "20"
                - --min-p
                - "0"
                - --no-mmap
                - --jinja
                - --reasoning-format
                - deepseek
                - --model
                - "/models/gguf/Qwen3-32B-128K-UD-Q8_K_XL.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-32B"
            - name: gemma3-27b
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - -c
                - "128000" 
                - -sm 
                - row
                - --temp
                - "0.6"
                - --top-p
                - "0.95"
                - --top-k
                - "64"
                - --min-p
                - "0"
                - --repeat-penalty 
                - "1.0"
                - --no-mmap
                - --jinja
                - --reasoning-format
                - deepseek
                - --mmproj
                - /models/gguf/gemma3/mmproj-F16.gguf
                - --model
                - "/models/gguf/gemma3/gemma-3-27b-it-UD-Q8_K_XL.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "gemma3-27b"
            - name: qwen-30b-coder
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - -fa
                - --cache-type-k
                - "q8_0"
                - --cache-type-v
                - "q8_0"
                - -c
                - "262144" 
                - -sm 
                - row
                - --temp
                - "0.7"
                - --top-p
                - "0.8"
                - --top-k
                - "20"
                - --repeat-penalty
                - "1.05"
                - --no-mmap
                - --jinja
                - --model
                - "/models/gguf/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-30b-coder"

            - name: qwen-30b-thinking
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - -fa
                - --cache-type-k
                - "q8_0"
                - --cache-type-v
                - "q8_0"
                - -c
                - "262144" 
                - -sm 
                - row
                - --temp
                - "0.6"
                - --top-p
                - "0.95"
                - --top-k
                - "20"
                - --min-p
                - "0"
                - --presence-penalty
                - "0"
                - --no-mmap
                - --jinja
                - --model
                - "/models/gguf/Qwen3-30B-A3B-Thinking-2507-UD-Q6_K_XL.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-30b-thinking"
            - name: qwen-30b-instruct
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - -fa
                - --cache-type-k
                - "q8_0"
                - --cache-type-v
                - "q8_0"
                - -c
                - "262144" 
                - -sm 
                - row
                - --temp
                - "0.7"
                - --top-p
                - "0.80"
                - --top-k
                - "20"
                - --min-p
                - "0"
                - --presence-penalty
                - "0"
                - --no-mmap
                - --jinja
                - --model
                #- "/models/gguf/Qwen3-30B-A3B-Instruct-2507-UD-Q8_K_XL.gguf"
                - "/models/gguf/Qwen3-30B-A3B-Instruct-2507-UD-Q6_K_XL.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-30b-instruct"
            - name: qwen-30b
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - -c
                - "40960" 
                - -n
                - "32768"
                - -sm 
                - row
                - --rope-scaling 
                - yarn 
                - --rope-scale
                - "4"
                - --yarn-orig-ctx
                - "32768"
                - --temp
                - "0.6"
                - --top-p
                - "0.95"
                - --top-k
                - "20"
                - --min-p
                - "0"
                - --no-mmap
                - --jinja
                - --reasoning-format
                - deepseek
                - --model
                - "/models/gguf/Qwen3-30B-A3B-128K-UD-Q8_K_XL.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-30B"
            - name: qwen-72b
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 5
                limits:
                  nvidia.com/gpu: 5
              args:
                - --rope-scaling 
                - yarn 
                - --rope-scale
                - "4"
                - --yarn-orig-ctx
                - "32768"
                - --temp
                - "0.7"
                - --top-p
                - "0.8"
                - --top-k
                - "20"
                - --no-mmap
                - --jinja
                - --mmproj
                - "/models/gguf/Qwen-72B/mmproj-Qwen2-VL-72B-Instruct-f32.gguf"
                - "-m"
                - "/models/gguf/Qwen-72B/Qwen2.5-VL-72B-Instruct-UD-Q6_K_XL-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -c
                - "131072"
                - -t
                - "-1"
                - --jinja
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "qwen-72b"
            - name: forgotten
              cooldownPeriod: 600
              resources:
                requests:
                  nvidia.com/gpu: 5
                limits:
                  nvidia.com/gpu: 5
              args:
                - -sm 
                - row
                - --rope-scaling 
                - yarn 
                - --rope-scale
                - "8"
                - --yarn-orig-ctx
                - "8192"
                - --no-mmap
                - --jinja
                - "-m"
                - "/models/gguf/forgotten/forgotten.gguf"
                - "-ngl"
                - "99"
                - -fa
                - -t
                - "-1"
                - --jinja
                - --prio
                - "3"
                - --ctx-size 
                - "65536"
                - --numa
                - distribute
                - --alias
                - "forgotten"
            - name: scout
              cooldownPeriod: 600
              resources:
                requests:
                  nvidia.com/gpu: 5
                limits:
                  nvidia.com/gpu: 5
              args:
                - --no-mmap
                - --jinja
                - "-m"
                - "/models/gguf/scout/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -fa
                - --cache-type-k
                - "q8_0"
                - --cache-type-v
                - "q8_0"
                - -t
                - "-1"
                - --jinja
                - --prio
                - "3"
                - --temp
                - "0.6"
                - --min-p
                - "0.01"
                - --top-p
                - "0.9"
                - --ctx-size 
                - "32000"
                - --mmproj
                - /models/gguf/scout/mmproj-F16.gguf
                - --numa
                - distribute
                - --alias
                - "scout"
            - name: qwen-235b-exl
              engine: tabby
              replicas: 1
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 5
                limits:
                  nvidia.com/gpu: 5
              args:
                - main.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --cuda-malloc-backend
                - "true"
                - --cache-mode
                - "Q8" 
                - --max-seq-len 
                - "55000" 
                - --model-dir
                - /models/exl2
                - --model-name
                - Qwen3-235B-A22B-Thinking-2507-exl3 
                - --disable-auth
                - "true"
            - name: qwen3-235b-q8
              cooldownPeriod: 3600
              device: cpu
              args:
                - -t
                - "-1"
                - --jinja
                - -c
                - "131072"
                - -ngl
                - "0"
                - --no-mmap
                - --numa
                - distribute
                - --rope-scaling 
                - yarn 
                - --rope-scale 
                - "4" 
                - --yarn-orig-ctx
                - "32768"
                - -m
                - /models/gguf/Qwen-235B/Qwen3-235B-A22B-128K-UD-Q8_K_XL-00001-of-00006.gguf
              selector: { }
              cpuResources:
                requests:
                  memory: "300Gi"
                limits:
                  memory: "300Gi"
  destination:
    server: https://kubernetes.default.svc
    namespace: inference
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - RespectIgnoreDifferences=true
      - ServerSideApply=true
