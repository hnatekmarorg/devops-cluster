apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-test
  labels:
    scale.off-replicas: "0"
    scale.on-replicas: "1"
    llm: vllm-test
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      llm: vllm-test
  template:
    metadata:
      name: vllm-test
      labels:
        llm: vllm-test
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      runtimeClassName: nvidia
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
        - name: test
          image: lmsysorg/sglang:v0.5.1.post3-cu128-b200 # ghcr.io/ggml-org/llama.cpp:full-cuda
          imagePullPolicy: Always
          resources:
            requests:
              nvidia.com/gpu: 6
            limits:
              nvidia.com/gpu: 6
          command: ["bash"]
          args: ["-c", "sleep infinity"]
          volumeMounts:
            - mountPath: /models
              name: nas
            - mountPath: /dev/shm
              name: shm
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
      restartPolicy: Always
      securityContext: {}