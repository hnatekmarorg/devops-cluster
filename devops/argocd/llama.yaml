apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llama-cpp
  namespace: argocd  # or your Argo CD namespace
spec:
  project: default
  ignoreDifferences:
    - kind: "*"
      group: "*"
      namespace: inference
      jsonPointers:
        - /spec/replicas
  sources:
    - name: chart
      repoURL: https://github.com/hnatekmarorg/devops-cluster.git
      path: charts/llama-cpp
      targetRevision: main
      helm:
        valuesObject:

          models:
            - name: exl
              cooldownPeriod: 3600
              engine: tabby
              replicas: 1
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - main.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --model-dir
                - /models/exl2
                - --inline-model-loading
                - "true"
                - --disable-auth
                - "true"
            - name: cogito
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --jinja
                - -c
                - "256000" 
                - --temp
                - "0.6"
                - --top-p
                - "0.9"
                - --min-p
                - "0.01"
                - --no-mmproj
                - --no-mmap
                - --model
                - "/models/gguf/cogito/cogito-v2-preview-llama-109B-MoE-UD-Q6_K_XL-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -fa
                - "on"
                - -t
                - "-1"
                - --prio
                - "3"
                - --jinja
                - --numa
                - distribute
                - --alias
                - "cogito"
            - name: gpt-oss
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - --jinja
                - -c
                - "131072" 
                - --temp
                - "1.0"
                - --top-p
                - "1"
                - --top-k
                - "0"
                - --no-mmap
                - --model
                - "/models/gguf/gpt-oss/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -fa
                - "on"
                - -t
                - "-1"
                - --prio
                - "3"
                - --jinja
                - --chat-template-kwargs 
                - '{"reasoning_effort": "high"}'
                - --numa
                - distribute
                - --alias
                - "gpt-oss"
                
            - name: gpt-oss-20b
              engine: vllm
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 1
                limits:
                  nvidia.com/gpu: 1
              args:
                - /models/fp8/gpt-oss-20b
                - --async-scheduling
                - --tool-call-parser 
                - openai
                - --enable-auto-tool-choice
                - --served-model-name
                - gpt-oss-20b
            - name: glm-4-6
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --ctx-size
                - "106000"
                - --top-p
                - "0.95"
                - --top-k
                - "40"
                - --no-mmap
                - --jinja
                - -m
                - "/models/gguf/glm-4.6/GLM-4.6-UD-IQ1_S-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - glm-4-6
            - name: behemoth
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --ctx-size
                - "131072"
                - --no-mmap
                - "-m"
                - "/models/gguf/behemoth/q_4/TheDrummer_Behemoth-R1-123B-v2-Q4_K_M-00001-of-00002.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "behemoth"
            - name: behemoth-redux
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --ctx-size
                - "64000"
                - --no-mmap
                - "-m"
                - "/models/gguf/behemoth-redux/TheDrummer_Behemoth-ReduX-123B-v1-Q6_K-00001-of-00003.gguf"
                - "-ngl"
                - "99"
                - -t
                - "-1"
                - --prio
                - "3"
                - --numa
                - distribute
                - --alias
                - "behemoth-redux"


            - name: qwen3-30b-coder
              engine: vllm
              cooldownPeriod: 7200
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - /models/awq/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit
                - --dtype 
                - float16
                - --pipeline-parallel-size
                - "2"
                - --enable-expert-parallel
                - --enable-auto-tool-choice 
                - --max-model-len
                - "252000"
                - --tool-call-parser
                - qwen3_coder
                - --served-model-name
                - qwen3-30b-coder
            - name: qwen3-235b-thinking
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --jinja
                - --parallel
                - "4"
                - -m 
                - /models/gguf/qwen3-235B/Qwen3-235B-A22B-Thinking-2507-UD-Q2_K_XL-00001-of-00002.gguf 
                - --ctx-size 
                - "226000"
                - "-ngl"
                - "99"
                - -fa
                - "on"
                - -t
                - "-1"
                - --prio
                - "3"
                - --jinja
                - --numa
                - distribute
                - --alias
                - "qwen3-235b-thinking"
            - name: qwen3-235b-instruct
              cooldownPeriod: 3600
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - --jinja
                - --parallel
                - "4"
                - -m 
                - /models/gguf/qwen3-235B/Qwen3-235B-A22B-Instruct-2507-UD-Q2_K_XL-00001-of-00002.gguf
                - --ctx-size 
                - "226000"
                - "-ngl"
                - "99"
                - -fa
                - "on"
                - -t
                - "-1"
                - --prio
                - "3"
                - --jinja
                - --numa
                - distribute
                - --alias
                - "qwen3-235b-instruct"
            - name: qwen3-30b-instruct
              engine: vllm
              cooldownPeriod: 7200
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - /models/fp8/Qwen3-VL-30B-A3B-Instruct-FP8/    
                - --async-scheduling 
                - --mm-encoder-tp-mode 
                - data
                - --tensor-parallel-size
                - "4"
                - --mm-processor-cache-type
                - shm
                - --enable-expert-parallel
                - --tool-call-parser 
                - hermes
                - --enable-auto-tool-choice
                - --served-model-name
                - qwen3-30b-instruct
            - name: qwen3-30b-thinking
              engine: vllm
              cooldownPeriod: 7200
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - /models/fp16/Qwen3-VL-30B-A3B-Thinking/
                - --max-model-len
                - "132000"
                - --async-scheduling 
                - --mm-encoder-tp-mode 
                - data
                - --tensor-parallel-size
                - "4"
                - --mm-processor-cache-type
                - shm
                - --enable-expert-parallel
                - --reasoning-parser 
                - deepseek_r1
                - --tool-call-parser 
                - hermes
                - --enable-auto-tool-choice
                - --served-model-name
                - qwen3-30b-thinking
  
            - name: qwen-next
              engine: vllm
              cooldownPeriod: 14400
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - /models/fp8/Qwen3-Next-80B-A3B-Thinking-FP8/ 
                - --pipeline-parallel-size
                - "3"
                - --tensor-parallel-size
                - "2" 
                - --reasoning-parser 
                - deepseek_r1
                - --tool-call-parser 
                - hermes 
                - --enable-auto-tool-choice
                - --no-enable-chunked-prefill
                - --tokenizer-mode
                - auto  
                - --gpu-memory-utilization 
                - "0.8"
                - --served-model-name
                - qwen-next
            - name: qwen-next-instruct
              engine: vllm
              cooldownPeriod: 14400
              resources:
                requests:
                  nvidia.com/gpu: 6
                limits:
                  nvidia.com/gpu: 6
              args:
                - /models/fp8/Qwen3-Next-80B-A3B-Instruct-FP8/ 
                - --pipeline-parallel-size
                - "3"
                - --tensor-parallel-size
                - "2" 
                - --tool-call-parser 
                - hermes 
                - --enable-auto-tool-choice
                - --no-enable-chunked-prefill
                - --tokenizer-mode
                - auto  
                - --gpu-memory-utilization 
                - "0.8"
                - --served-model-name
                - qwen-next-instruct
  destination:
    server: https://kubernetes.default.svc
    namespace: inference
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - RespectIgnoreDifferences=true
      - ServerSideApply=true
