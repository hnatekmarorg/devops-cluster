llm:
  host: llm.hnatekmar.dev
  vllm:
    image: vllm/vllm-openai:v0.10.2
  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    cpuImage: ghcr.io/ggml-org/llama.cpp:server

ingress:
  host: llama.hnatekmar.dev

models:
  - name: qwen3
    args:
      - --jinja
      - "-m"
      - "/models/gguf/Qwen3-32B-Q5_K_M.gguf"
      - "-ngl"
      - "99"
    selector: {}
    replicas: 3
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    cooldownPeriod: 120
  - name: qwen3
    args:
      - --jinja
      - "-m"
      - "/models/gguf/Qwen3-32B-Q5_K_M.gguf"
      - "-ngl"
      - "99"
    selector: {}
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    cooldownPeriod: 120
  # Example tabbyapi model configuration
  - name: tabby-qwen3
    engine: tabby
    replicas: 1
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - start.py
      - --model-dir
      - /models/exl2
      - --model-name
      - Qwen2.5-Coder-32B-Instruct-exl2
      - --gpu-lib
      - cu121
      - --disable-auth
      - "true"
    cooldownPeriod: 120

  # CPU model example
  - name: qwen3-cpu
    device: cpu
    args:
      - --jinja
      - "-m"
      - "/models/gguf/Qwen3-32B-Q5_K_M.gguf"
      # Note: Removed "-ngl" parameter for CPU operation
    cpuResources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    replicas: 3
    cooldownPeriod: 120
