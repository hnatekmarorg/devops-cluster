{{ range  .Values.models }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{.name}}
  labels:
    llm: {{.name}}
    scale.off-replicas: "0"
    scale.on-replicas: "1"
spec:
  replicas: 0
  selector:
    matchLabels:
      llm: {{.name}}
  template:
    metadata:
      name: {{.name}}
      labels:
        llm: {{.name}}
    spec:
      #hostIPC: true
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      {{- if eq (.device | default "gpu") "gpu" }}
      runtimeClassName: nvidia
      {{- end }}
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
        - name: llamacpp
          securityContext:
            privileged: false        
            capabilities:
              add: ["IPC_LOCK"]      
          resources:
            {{- if eq (.device | default "gpu") "gpu" }}
            {{toYaml .resources | nindent 12}}
            {{- else }}
            {{toYaml .cpuResources | nindent 12}}
            {{- end }}
          {{- if .image }}
          image: {{ .image }}
          {{- else if eq (.engine | default "llama") "vllm" }}
          image: {{ $.Values.llm.vllm.image }}
          {{- else if eq (.engine | default "llama") "tabby" }}
          image: ghcr.io/theroyallab/tabbyapi:latest
          {{- else }}
          {{- if eq (.device | default "gpu") "gpu" }}
          image: {{ $.Values.llm.llama_cpp.image }}
          {{- else }}
          image: {{ $.Values.llm.llama_cpp.cpuImage }}
          {{- end }}
          {{- end }}
          imagePullPolicy: Always
          {{- if eq (.engine | default "llama") "vllm" }}
          command: ["/bin/bash"]
          {{- end }}
          env:
            {{- if .env }}
            {{- range .env }}
            - name: {{ .name }}
              value: {{ .value | quote }}
            {{- end }}
            {{ end -}}
          {{- if eq (.engine | default "llama") "vllm" }}
          args: 
            - -c 
            - "/usr/local/bin/vllm serve {{ join " " .args }} --host 0.0.0.0 --port 8080"
          {{- else }}
          args: {{ toYaml .args | nindent 12}} 
          {{- end }}
          volumeMounts:
            - mountPath: /models
              name: nas
            - mountPath: /dev/shm
              name: shm
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
      restartPolicy: Always
      securityContext: {}
---
apiVersion: v1
kind: Service
metadata:
  name: {{.name}}
spec:
  selector:
    llm: {{.name}}
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP
---
{{end}}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{$.Values.llm.host}}
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    cert-manager.io/cluster-issuer: letsencrypt-cloudflare
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "3600"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - {{$.Values.llm.host}}
      secretName: {{$.Values.llm.host}}
  rules:
{{ range  .Values.models }}
    - host: {{$.Values.llm.host}}
      http:
        paths:
          - path: /{{.name}}(/|$)(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: {{.name}}
                port:
                  number: 8080
{{end}}
