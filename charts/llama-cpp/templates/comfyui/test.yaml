apiVersion: apps/v1
kind: Deployment
metadata:
  name: lamacpp-test
  labels:
    scale.off-replicas: "0"
    scale.on-replicas: "1"
    llm: vllm-test
spec:
  strategy:
    type: Recreate
  replicas: 0
  selector:
    matchLabels:
      llm: lamacpp-test
  template:
    metadata:
      name: lamacpp-test
      labels:
        llm: lamacpp-test
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      runtimeClassName: nvidia
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
        - name: test
          image: ghcr.io/ggml-org/llama.cpp:full-cuda
          imagePullPolicy: Always
          resources:
            requests:
              nvidia.com/gpu: 6
            limits:
              nvidia.com/gpu: 6
          command: ["bash"]
          args: ["-c", "sleep infinity"]
          volumeMounts:
            - mountPath: /models
              name: nas
            - mountPath: /dev/shm
              name: shm
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
      restartPolicy: Always
      securityContext: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-test
  labels:
    scale.off-replicas: "0"
    scale.on-replicas: "1"
    llm: vllm-test
spec:
  strategy:
    type: Recreate
  replicas: 0
  selector:
    matchLabels:
      llm: vllm-test
  template:
    metadata:
      name: vllm-test
      labels:
        llm: vllm-test
    spec:
      hostIPC: true
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      runtimeClassName: nvidia
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
        - name: test
          image: vllm/vllm-openai:v0.12.0
          imagePullPolicy: Always
          securityContext:
            privileged: true
          resources:
            requests:
              nvidia.com/gpu: 6
            limits:
              nvidia.com/gpu: 6
          command: ["bash"]
          args: ["-c", "sleep infinity"]
          volumeMounts:
            - mountPath: /models
              name: nas
            - mountPath: /dev/shm
              name: shm
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
      restartPolicy: Always
      securityContext: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sglang-test
  labels:
    scale.off-replicas: "0"
    scale.on-replicas: "1"
    llm: vllm-test
spec:
  strategy:
    type: Recreate
  replicas: 0
  selector:
    matchLabels:
      llm: sglang-test
  template:
    metadata:
      name: sglang-test
      labels:
        llm: sglang-test
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      runtimeClassName: nvidia
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
        - name: test
          image: lmsysorg/sglang:v0.5.5.post3-cu129-amd64
          imagePullPolicy: Always
          resources:
            requests:
              nvidia.com/gpu: 6
            limits:
              nvidia.com/gpu: 6
          command: ["bash"]
          args: ["-c", "sleep infinity"]
          volumeMounts:
            - mountPath: /models
              name: nas
            - mountPath: /dev/shm
              name: shm
          ports:
            - containerPort: 8080
              protocol: TCP
              name: http
      restartPolicy: Always
      securityContext: {}

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-test
spec:
  replicas: 0
  selector:
    matchLabels:
      app: llamacpp-test-gpu
  template:
    metadata:
      labels:
        app: llamacpp-test
        scale.off-replicas: "0"
        scale.on-replicas: "1"
    spec:
      runtimeClassName: nvidia
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
        - name: nas
          nfs:
            server: 192.168.88.25
            path: /mnt/data/k8s/ai
      containers:
      - name: llamacpp-test
        image: ghcr.io/ggml-org/llama.cpp:full-cuda
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
        command: ["bash"]
        args: ["-c", "sleep infinity"]
        ports:
          - containerPort: 8080
            protocol: TCP
            name: http
        volumeMounts:
          - mountPath: /models
            name: nas
          - mountPath: /dev/shm
            name: shm
      restartPolicy: Always
      securityContext: {}
