apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-proxy
data:
  litellm_config.yaml: |
    model_list:
    {{range .Values.models}}
      - model: openai/{{.name}}
        api_base: {{printf "https://%s/%s/v1" $.Values.llm.host .name }}
        api_key: sk-dummy
    {{end}}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-swap
  labels:
    app: llama-swap
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-swap
  template:
    metadata:
      name: llama-swap
      labels:
        app: llama-swap
    spec:
      volumes:
        - name: config
          configMap:
            name: llm-proxy
            items:
              - key: litellm_config.yaml
                path: litellm_config.yaml
      containers:
        - name: llm-proxy
          image: ghcr.io/berriai/litellm:main-stable
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /mnt
              name: config
          args:
            - --config
            - /mnt/litellm_config.yaml
            - --port
            - "8080"
          ports:
            - containerPort: 8080
              protocol: TCP
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: llama-swap
spec:
  selector:
    app: llama-swap
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama-swap
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-cloudflare
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - {{.Values.ingress.host}}
      secretName: llama-swap
  rules:
    - host: {{.Values.ingress.host}}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: llama-swap
                port:
                  number: 8080
