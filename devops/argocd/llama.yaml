apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llama-cpp
  namespace: argocd  # or your Argo CD namespace
spec:
  project: default
  ignoreDifferences:
    - kind: "*"
      group: "*"
      namespace: inference
      jsonPointers:
        - /spec/replicas
  sources:
    - name: chart
      repoURL: https://github.com/hnatekmarorg/devops-cluster.git
      path: charts/llama-cpp
      targetRevision: main
      helm:
        valuesObject:
          models:
            - name: devstral
              replicas: 1
              cooldownPeriod: 600
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - --flash-attn
                - --jinja
                - -c
                - "128000"
                - -ngl
                - "99"
                - "--host"
                - "0.0.0.0"
                - "--port"
                - "8080"
                - --model
                - /models/gguf/Devstral-Small-2505-UD-Q8_K_XL.gguf
            - name: qwen-coder-exl2
              engine: tabby
              replicas: 1
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - start.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --max-seq-len 
                - "32768"
                - --model-dir
                - /models/exl2
                - --model-name
                - Qwen2.5-Coder-32B-Instruct-exl2
                - --gpu-lib
                - cu121
                - --disable-auth
                - "true"
              cooldownPeriod: 120
            - name: gemma-3-27b # exl2 example
              engine: tabby
              replicas: 1
              cooldownPeriod: 120
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              args:
                - start.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --max-seq-len 
                - "128000"
                - --model-dir
                - /models/exl2
                - --model-name
                - "/models/exl2/gemma-3-27b-it-exl2"
                - --gpu-lib
                - cu121
                - --disable-auth
                - "true"
            - name: fallen-abomination
              engine: tabby
              replicas: 1
              cooldownPeriod: 240
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - start.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --max-seq-len 
                - "90000"
                - --model-dir
                - /models/exl2
                - --model-name
                - "Fallen-Abomination-70B-R1-v4.1_EXL2_4.65bpw_H8"
                - --gpu-lib
                - cu121
                - --disable-auth
                - "true"
            - name: fallen-llama
              engine: tabby
              replicas: 1
              cooldownPeriod: 240
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
              args:
                - start.py
                - --host
                - "0.0.0.0"
                - --port
                - "8080"
                - --max-seq-len 
                - "90000"
                - --model-dir
                - /models/exl2
                - --model-name
                - "Fallen-Llama-3.3-R1-70B-v1-6.0bpw-h8-exl2"
                - --gpu-lib
                - cu121
                - --disable-auth
                - "true"
            - name: agatha
              args:
                - -c
                - "64000"
                - -ngl
                - "99"
                - --flash-attn
                - --numa
                - distribute
                - -m
                - /models/gguf/agatha/TheDrummer_Agatha-111B-v1-Q4_K_S-00001-of-00002.gguf
              selector: { }
              resources:
                requests:
                  nvidia.com/gpu: 3
                limits:
                  nvidia.com/gpu: 3
            - name: fallen-command
              args:
                - -c
                - "64000"
                - -ngl
                - "99"
                - --flash-attn
                - --numa
                - distribute
                - -m
                - /models/gguf/fallen-command/TheDrummer_Fallen-Command-A-111B-v1.1-Q4_K_S-00001-of-00002.gguf
              selector: { }
              resources:
                requests:
                  nvidia.com/gpu: 3
                limits:
                  nvidia.com/gpu: 3

            - name: qwen3-235b
              args:
                - --jinja
                - -c
                - "32768"
                - -ngl
                - "99"
                - --no-mmap
                - --flash-attn
                - --numa
                - distribute
                - --override-tensor
                - blk\.[0-9][0-7]?\.ffn.*exps.*=CPU
                - -m
                - /models/gguf/Qwen-235B/Qwen3-235B-A22B-128K-UD-Q8_K_XL-00001-of-00006.gguf
              selector: { }
              resources:
                requests:
                  memory: "300Gi"
                limits:
                  memory: "300Gi"
            - name: qwen3-32b
              args:
                - --jinja
                - "-m"
                - "/models/gguf/Qwen3-32B-Q5_K_M.gguf"
                - "-ngl"
                - "99"
              selector: { }
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
            - name: qwen-72b
              args:
                - "--jinja"
                - "-c"
                - "32768"
                - "--mmproj"
                - "/models/gguf/Qwen-72B/mmproj-Qwen2-VL-72B-Instruct-f32.gguf"
                - "-m"
                - "/models/gguf/Qwen-72B/Qwen2.5-VL-72B-Instruct-UD-Q5_K_XL-00001-of-00002.gguf"
                - "-ngl"
                - "99"
              selector: { }
              resources:
                requests:
                  nvidia.com/gpu: 4
                limits:
                  nvidia.com/gpu: 4
  destination:
    server: https://kubernetes.default.svc
    namespace: inference
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - RespectIgnoreDifferences=true
      - ServerSideApply=true
