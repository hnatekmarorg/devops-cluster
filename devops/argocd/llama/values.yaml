# Llama models configuration
# Each model defines GPU resources, arguments, and other settings
models:
  # ExL model configuration
  - name: exl
    cooldownPeriod: 3600
    engine: tabby
    replicas: 1
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - main.py
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model-dir
      - /models/exl2
      - --inline-model-loading
      - "true"
      - --disable-auth
      - "true"
  # Cogito model configuration  
  - name: cogito
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --jinja
      - -c
      - "256000" 
      - --temp
      - "0.6"
      - --top-p
      - "0.9"
      - --min-p
      - "0.01"
      - --no-mmproj
      - --no-mmap
      - --model
      - "/models/gguf/cogito/cogito-v2-preview-llama-109B-MoE-UD-Q6_K_XL-00001-of-00002.gguf"
      - "-ngl"
      - "99"
      - -fa
      - "on"
      - -t
      - "-1"
      - --prio
      - "3"
      - --jinja
      - --numa
      - distribute
      - --alias
      - "cogito"
  # GPT-OSS model configuration
  - name: gpt-oss
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - --jinja
      - -c
      - "131072" 
      - --temp
      - "1.0"
      - --top-p
      - "1"
      - --top-k
      - "0"
      - --no-mmap
      - --model
      - "/models/gguf/gpt-oss/gpt-oss-120b-UD-Q8_K_XL-00002-of-00002.gguf"
      - "-ngl"
      - "99"
      - -fa
      - "on"
      - -t
      - "-1"
      - --prio
      - "3"
      - --jinja
      - --chat-template-kwargs 
      - '{"reasoning_effort": "high"}'
      - --numa
      - distribute
      - --alias
      - "gpt-oss"
  # GPT-OSS 20B model configuration
  - name: gpt-oss-20b
    engine: vllm
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp8/gpt-oss-20b
      - --async-scheduling
      - --tool-call-parser 
      - openai
      - --enable-auto-tool-choice
      - --served-model-name
      - gpt-oss-20b
  # GLM-4.6 model configuration
  - name: glm-4-6
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "106000"
      - --top-p
      - "0.95"
      - --top-k
      - "40"
      - --no-mmap
      - --jinja
      - -m
      - "/models/gguf/glm-4.6/GLM-4.6-UD-IQ1_S-00001-of-00002.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - distribute
      - --alias
      - glm-4-6
  # Behemoth model configuration
  - name: behemoth
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "80000"
      - --no-mmap
      - "-m"
      - "/models/gguf/behemoth-r1/TheDrummer_Behemoth-R1-123B-v2-Q5_K_M-00001-of-00003.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - distribute
      - --alias
      - "behemoth"
  # Behemoth Redux model configuration
  - name: behemoth-redux
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --ctx-size
      - "64000"
      - --no-mmap
      - "-m"
      - "/models/gguf/behemoth-redux/TheDrummer_Behemoth-ReduX-123B-v1-Q6_K-00001-of-00003.gguf"
      - "-ngl"
      - "99"
      - -t
      - "-1"
      - --prio
      - "3"
      - --numa
      - distribute
      - --alias
      - "behemoth-redux"
  # GLM-Air model configuration
  - name: glm-air
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/awq/GLM-4.5-Air-AWQ-4bit/
      - --tensor-parallel-size
      - "2"
      - --pipeline-parallel-size
      - "2"
      - --enable-expert-parallel
      - --enable-auto-tool-choice 
      - --max-model-len
      - "126000"
      - --tool-call-parser
      - glm45
      - --reasoning-parser
      - glm45
      - --served-model-name
      - glm-air
  # Qwen3-235B Thinking model configuration
  - name: qwen3-235b-thinking
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --jinja
      - --parallel
      - "4"
      - -m 
      - /models/gguf/qwen3-235B/Qwen3-235B-A22B-Thinking-2507-UD-Q2_K_XL-00001-of-00002.gguf 
      - --ctx-size 
      - "226000"
      - "-ngl"
      - "99"
      - -fa
      - "on"
      - -t
      - "-1"
      - --prio
      - "3"
      - --jinja
      - --numa
      - distribute
      - --alias
      - "qwen3-235b-thinking"
  # Qwen3-235B Instruct model configuration
  - name: qwen3-235b-instruct
    cooldownPeriod: 3600
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - --jinja
      - --parallel
      - "4"
      - -m 
      - /models/gguf/qwen3-235B/Qwen3-235B-A22B-Instruct-2507-UD-Q2_K_XL-00001-of-00002.gguf
      - --ctx-size 
      - "226000"
      - "-ngl"
      - "99"
      - -fa
      - "on"
      - -t
      - "-1"
      - --prio
      - "3"
      - --jinja
      - --numa
      - distribute
      - --alias
      - "qwen3-235b-instruct"
  # Qwen3-30B Instruct model configuration
  - name: qwen3-30b-instruct
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
    args:
      - /models/awq/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit/ 
      - --gpu-memory-utilization 
      - "0.95"
      - --max-model-len
      - "180000"
      - --async-scheduling 
      - --tensor-parallel-size
      - "2"
      - --mm-processor-cache-type
      - shm
      - --enable-expert-parallel
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-instruct
  # Qwen3-30B Thinking model configuration
  - name: qwen3-30b-thinking
    engine: vllm
    cooldownPeriod: 7200
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
    args:
      - /models/awq/Qwen3-VL-30B-A3B-Thinking-AWQ-4bit/
      - --async-scheduling 
      - --max-model-len
      - "180000"
      - --gpu-memory-utilization 
      - "0.95"
      - --mm-encoder-tp-mode 
      - data
      - --tensor-parallel-size
      - "2"
      - --mm-processor-cache-type
      - shm
      - --enable-expert-parallel
      - --reasoning-parser 
      - deepseek_r1
      - --tool-call-parser 
      - hermes
      - --enable-auto-tool-choice
      - --served-model-name
      - qwen3-30b-thinking
  # Qwen Coder model configuration
  - name: qwen-coder
    engine: vllm
    cooldownPeriod: 14400
    resources:
      requests:
        nvidia.com/gpu: 4
      limits:
        nvidia.com/gpu: 4
    args:
      - /models/fp8/Qwen3-Coder-30B-A3B-Instruct-FP8 --gpu-memory-util 0.8 --tensor-parallel-size 4 --served-model-name qwen-coder --enable-expert-parallel   --enable-auto-tool-choice --tool-call-parser qwen3_coder
  # Qwen Next Instruct model configuration
  - name: qwen-next-instruct
    engine: vllm
    cooldownPeriod: 14400
    resources:
      requests:
        nvidia.com/gpu: 6
      limits:
        nvidia.com/gpu: 6
    args:
      - /models/awq/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit/
      - --tensor-parallel-size
      - "4" 
      - --tool-call-parser 
      - hermes 
      - --enable-auto-tool-choice
      - --no-enable-chunked-prefill
      - --tokenizer-mode
      - auto  
      - --gpu-memory-utilization 
      - "0.8"
      - --served-model-name
      - qwen-next-instruct
  # Qwen Embedding model configuration
  - name: qwen-embedding
    engine: vllm
    cooldownPeriod: 14400
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp16/Qwen3-Embedding-8B/
      - --served-model-name
      - qwen-embedding
  # Qwen Reranker model configuration
  - name: qwen-reranker
    engine: vllm
    cooldownPeriod: 14400
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    args:
      - /models/fp16/Qwen3-Reranker-8B/
      - --max-model-len
      - "30000"
      - --served-model-name
      - qwen-reranker