llm:
  host: llm.hnatekmar.dev
  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-b5664

ingress:
  host: llama.hnatekmar.dev

models:
  - name: qwen3
    args:
      - --jinja
      - "-m"
      - "/models/gguf/Qwen3-32B-Q5_K_M.gguf"
      - "-ngl"
      - "99"
    selector: {}
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
